# templates/code_templates.py

STATIC_IMPORTS = """#!/usr/bin/env python3
\"\"\"
Azure Data Factory Pipeline - Auto-Generated by AI Agent System
This file was generated by Azure OpenAI Agents
Generated on: {timestamp}
\"\"\"

import os
import time
from datetime import datetime
from azure.identity import ClientSecretCredential
from azure.mgmt.datafactory import DataFactoryManagementClient
from azure.mgmt.datafactory.models import *
"""

STATIC_CLASS_HEADER = """
class AutoGeneratedADFPipeline:
    \"\"\"
    Auto-generated ADF Pipeline Class
    Created by Azure OpenAI Agent System
    \"\"\"
    
    def __init__(self, subscription_id, resource_group, factory_name, location='eastus', 
                 use_timestamp=False, tenant_id=None, client_id=None, client_secret=None):
        self.subscription_id = subscription_id
        self.resource_group = resource_group
        self.factory_name = factory_name
        self.location = location
        self.use_timestamp = use_timestamp
        
        self.tenant_id = tenant_id
        self.client_id = client_id
        self.client_secret = client_secret
        
        self.timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        self.names = self.generate_resource_names()
        
        self.credential = self.get_credential()
        self.client = DataFactoryManagementClient(self.credential, subscription_id)
    
    def generate_resource_names(self):
        suffix = f"{self.timestamp}" if self.use_timestamp else ""
        return {
            'sql_linked_service': f'SQLLinkedServiceConnection{suffix}',
            'blob_linked_service': f'AzureBlobStorageConnection{suffix}',
            'source_csv_dataset': f'SourceCSVDataset{suffix}',
            'staging_csv_dataset': f'StagingCSVDataset{suffix}',
        }
    
    def get_credential(self):
        if all([self.tenant_id, self.client_id, self.client_secret]):
            print(f"Using Service Principal authentication")
            return ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
        raise ValueError(
            "Azure credentials not provided. Pass tenant_id, client_id, and client_secret."
        )
"""

DATATYPE_CASTING_TEMPLATE = """
# Data Type Casting Configuration (Generated by Agent 2)
COLUMN_CASTING = {{
{casting_definitions}
}}
"""

DATAFLOW_TEMPLATE = """
def create_transform_dataflow(self):
    \"\"\"Transform data flow - Generated by Agent 3\"\"\"
    name = self.names.get('transform_dataflow', 'TransformDataFlow')
    print(f"Creating Transform Data Flow: {{name}}...")
    
    script = \"\"\"
{dataflow_script}
    \"\"\"
    
    dataflow_properties = MappingDataFlow(
        sources=[{sources}],
        sinks=[{sinks}],
        transformations=[{transformations}],
        script=script
    )
    
    dataflow = DataFlowResource(properties=dataflow_properties)
    
    result = self.client.data_flows.create_or_update(
        self.resource_group,
        self.factory_name,
        name,
        dataflow
    )
    print(f"âœ“ Transform Data Flow created: {{result.name}}")
    return result
"""

MAIN_EXECUTION_TEMPLATE = """
def main():
    # Azure Configuration
    SUBSCRIPTION_ID = '{subscription_id}'
    RESOURCE_GROUP = '{resource_group}'
    FACTORY_NAME = '{factory_name}'
    LOCATION = '{location}'
    
    TENANT_ID = '{tenant_id}'
    CLIENT_ID = '{client_id}'
    CLIENT_SECRET = '{client_secret}'
    
    # Initialize pipeline
    pipeline = AutoGeneratedADFPipeline(
        subscription_id=SUBSCRIPTION_ID,
        resource_group=RESOURCE_GROUP,
        factory_name=FACTORY_NAME,
        location=LOCATION,
        use_timestamp=False,
        tenant_id=TENANT_ID,
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET
    )
    
    # Deploy pipeline
    print("Deploying pipeline...")
    # Add deployment logic here
    
if __name__ == '__main__':
    main()
"""